<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hezhen Hu</title>
  
  <meta name="author" content="Hezhen Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="./images/ut.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hezhen Hu</name>
              </p>
              <p> I am a postdoctoral fellow in <a href="https://vita-group.github.io/index.html">VITA group</a> in <a href="https://www.utexas.edu/">University of Texas at Austin (UT Austin)</a>, supervised by Prof. <a href="https://scholar.google.com/citations?user=pxFyKAIAAAAJ&hl=zh-CN">Zhangyang Wang</a>. I also work closely with Prof. <a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>.
              </p>
              </p>
              <p>
              I got my PhD degree from <a href="https://en.ustc.edu.cn/">University of Science and Technology of China (USTC)</a>, supervised by Prof. <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a> and Prof. <a href="http://staff.ustc.edu.cn/~lihq/en/">Houqiang Li</a>. I served as a leader in the <a href="https://ustc-slr.github.io/">VSLRG</a> research team of sign language understanding.
              From May 2022 to Feburary 2023, I am fortune to work as a research intern at <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/">Microsfot Research Asia</a>, supervised by <a href="https://jianminbao.github.io/">Jianmin Bao</a>, <a href="https://www.dongdongchen.bid/">Dongdong Chen</a>, Lu Yuan, <a href="http://www.dongchen.pro/">Dong Chen</a>, and Fang Wen. 
              </p>
              </p>
              <p> I'm interested in human avatar modeling, 3D represenation, sign language understanding, large-scale pre-training, and etc. 
              </p>
              <p>
                <strong>Please feel free to contact me if you are interested in the above topics and want to collaborate with me.</strong>
              </p>

              <p style="text-align:center">
                <a href="mailto:alexhu@utexas.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp -->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=Fff-9WAAAAAJ">Google Scholar</a> 
                <!-- &nbsp/&nbsp -->
                <!-- <a href="https://github.com/AlexHu123">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Self.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Self.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li>2025.10:&nbsp;We are organzing the 3rd <a href="https://ai3dcc.github.io/">AI3DCC</a> workshop at ICCV 2025.</li>
              </ul>
            </td>
          </tr>
      </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
              <p>
                Representative papers are <span class="highlight">highlighted</span>.
                For the full list, please refer to <a href="https://scholar.google.com/citations?user=Fff-9WAAAAAJ&hl=zh-CN">Google Scholar</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/EVA.jpg" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2407.03204">
          <papertitle>Expressive Gaussian Human Avatars from Monocular RGB Video</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>,
        <a href="https://chikayan.github.io/">Tianhao Wu</a>,
        <a href="https://scholar.google.com/citations?user=34s2YS0AAAAJ&hl=en">Yihan Xi</a>,
        <a href="https://seoyoung1215.github.io/">Seoyoung Lee</a>,
        <a href="https://geopavlakos.github.io/">Georgios Pavlakos</a>,
        <a href="https://scholar.google.com/citations?user=pxFyKAIAAAAJ&hl=en">Zhangyang Wang</a>,
        <br>
        <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <a href="https://evahuman.github.io/">Project</a>
        <p><em><strong>Expressive</strong></em> animatable avatar (hand & face), learn from in-the-wild monocular RGB video (<em><strong>no SMPL-X annotation required</strong></em>). </p>
      </td>
    </tr>


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/SignBERT+.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2305.04868">
          <papertitle>SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="https://scholar.google.com/citations?user=v-ASmMIAAAAJ&hl=zh-CN">Weichao Zhao</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2023
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <a href="https://signbert-zoo.github.io/">Project</a>
        <p>[Extension of SignBERT] The <em><strong>first</strong></em> self-supervised pre-training framework in sign language understanding. </p>
      </td>
    </tr>


    <tr bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/HOIG.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2211.15663">
          <papertitle>Hand-Object Interaction Image Generation</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="https://scholar.google.com/citations?user=YfV4aCQAAAAJ&hl=zh-CN">Weilun Wang</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <a href="https://play-with-hoi-generation.github.io/">Project</a>
        <p>We present a new task, i.e., hand-object interaction image generation. This task is challenging and research-worthy in many potential application scenarios, such as online shopping. </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/MMHU.jpg" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2501.15187">
          <papertitle>MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding</papertitle>
        </a>
        <br>
        <a href="https://shadowiterator.github.io/">Renjie Li</a>,
        Ruijie Ye,
        Mingyang Wu,
        <a href="https://www.haofrankyang.net/">Hao (Frank) Yang</a>,
        <a href="https://zhiwenfan.github.io/">Zhiwen Fan</a>,
        <strong>Hezhen Hu</strong>,
        <a href="https://vztu.github.io/">Zhengzhong Tu</a>,
        <br>
        <em>Preprint
        <br>
        <p> We propose a large-scale generative pre-training strategy, that eliminates the gap between pre-training and downstream SLU tasks.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/UniSign.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2501.15187">
          <papertitle>Uni-Sign: Toward Unified Sign Language Understanding at Scale</papertitle>
        </a>
        <br>
        <a href="https://github.com/ZechengLi19">Zecheng Li</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="https://scholar.google.com/citations?user=v-ASmMIAAAAJ&hl=zh-CN">Weichao Zhao</a>,
        Kepeng Wu,
        <strong>Hezhen Hu</strong>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025
        <br>
        <p> We propose a large-scale generative pre-training strategy, that eliminates the gap between pre-training and downstream SLU tasks.</p>
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/PCMA.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://ieeexplore.ieee.org/document/10105511/">
          <papertitle>Prior-aware Cross Modality Augmentation Learning for Continuous Sign Language Recognition</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="http://home.ustc.edu.cn/~pjh/">Junfu Pu</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        Hang Fang,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>IEEE Transactions on Multimedia (<strong>TMM</strong>)</em>, 2023
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p> We propose a novel cross-modality augmentation learning paradigm with prior incorporated for continuous SLR.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/CoSLR.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://ieeexplore.ieee.org/document/9954921/">
          <papertitle>Collaborative Multilingual Sign Language Recognition: A Unified Framework</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="http://home.ustc.edu.cn/~pjh/">Junfu Pu</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>IEEE Transactions on Multimedia (<strong>TMM</strong>)</em>, 2022
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p>We are the <em><strong>first</strong></em> to explore the multilingual topic in continuous SLR and propose a unified framework targeting this problem. </p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/SignBERT.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Hu_SignBERT_Pre-Training_of_Hand-Model-Aware_Representation_for_Sign_Language_Recognition_ICCV_2021_paper.pdf">
          <papertitle>SignBERT: Pre-Training of Hand-Model-Aware Representation for Sign Language Recognition</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="https://scholar.google.com/citations?user=v-ASmMIAAAAJ&hl=zh-CN">Weichao Zhao</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p>The <em><strong>first</strong></em> self-supervised pre-training framework in isolated SLR. It conducts masked modeling with hand-prior incorporated for better capturing context in the sign language domain.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/MG2T.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Hu_Model-Aware_Gesture-to-Gesture_Translation_CVPR_2021_paper.pdf">
          <papertitle>Model-Aware Gesture-to-Gesture Translation</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="https://scholar.google.com/citations?user=YfV4aCQAAAAJ&hl=zh-CN">Weilun Wang</a>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="https://scholar.google.com/citations?user=v-ASmMIAAAAJ&hl=zh-CN">Weichao Zhao</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p>The <em><strong>first</strong></em> model-aware framework in gesture-to-gesture translation.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/HMA.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://ojs.aaai.org/index.php/AAAI/article/view/16247/16054">
          <papertitle>Hand-Model-Aware Sign Language Recognition</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, 2021
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p>The <em><strong>first</strong></em> model-aware framework in isolated SLR.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/NMFs-CSL.png" alt="b3do" width="260" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2008.10428">
          <papertitle>Global-local Enhancement Network for NMFs-aware Sign Language Recognition</papertitle>
        </a>
        <br>
        <strong>Hezhen Hu</strong>,
        <a href="http://staff.ustc.edu.cn/~zhwg/">Wengang Zhou</a>,
        <a href="http://home.ustc.edu.cn/~pjh/">Junfu Pu</a>,
        <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>,
        <br>
        <em>ACM Transactions on Multimedia Computing, Communications, and Applications (<strong>TOMM</strong>)</em>, 2021
        <br>
        <!-- <a href="data/B3DO_ICCV_2011.bib">bibtex</a> / -->
        <!-- <a href="https://drive.google.com/file/d/1qf4-U5RhSw12O7gzQwW66SMQhs2FWYDW/view?usp=sharing">"smoothing" code</a> -->
        <p>The <em><strong>first</strong></em> dataset explicitly exploring the importance of non-manual features in sign language.</p>
      </td>
    </tr>



        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Services</heading>
              <p>
                I am happy to serve as a reviewer. Please feel free to contact me. Till now, I have been invited to review the following Journals and Conferences: 
              </p>
              <ul>
                <li> IEEE Conference on Computer Vision and Pattern Recognition (CVPR) </li>
                <li> International Conference on Computer Vision (ICCV) </li>
                <li> European Conference on Computer Vision (ECCV) </li>
                <li> Conference on Neural Information Processing Systems (NeurIPS) </li>
                <li> International Conference on Learning Representations (ICLR) </li>
                <li> International Conference on Machine Learning (ICML) </li>
                <li> AAAI Conference on Artificial Intelligence (AAAI)  </li>
                <li> International Joint Conference on Artificial Intelligence (IJCAI) </li>
                <li> IEEE Transactions on Multimedia (TMM)  </li>
                <li> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) </li>
              </ul>
              </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

	
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
              <ul>
                <li> National Scholarship, Education Ministry of China, 2021, 2017, 2015 </li>
                <li> Presidential Scholarship, Chinese Academy of Sciences (CAS), 2023 </li>
                <li> "Stars of Tomorrow" Excellent Internship Award, MSRA, 2023 </li>
                <li> Outstanding Graduate Student of USTC, 2023  </li>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>

					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p align="middle"><font size="2">
                Last Update: January 21th, 2025 <br>
                Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. Thanks!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<div style="width:200px;margin:0 auto">
  <!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=jWKtanh3GLIaA7YarJE404P09JqFOL2zKwXMRyyAEBE&cl=ffffff&w=a"></script> -->
  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=jwZ8W1lDC1lgo8n-1JcyVVxUYRhyT6GKf8Z71oOxWb4"></script>
  </div>
</body>
</html>
